{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Introduction to RNN models in PyTorch.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://drive.google.com/uc?id=18iTizsKdS1VGN3Gx4J7LyYUTpnrqgx-Z\" />"
      ],
      "metadata": {
        "id": "xnDRFdCh3Xmu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Contents\n",
        "<ul>\n",
        "    <li><a href='#1'>Vanilla RNN</a></li>\n",
        "    <li><a href='#2'>LSTM</a></li>\n",
        "    <li><a href='#3'>GRU</a></li>\n",
        "    <li><a href='#4'>AutoEncoder & AutoDecoder</a></li>   \n",
        "</ul>"
      ],
      "metadata": {
        "id": "kHYG6hdC25tt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id=\"1\"><h2>Vanilla RNN model implementation</h2></div>\n",
        "Pytoch documentation: <a href=\"https://pytorch.org/docs/1.9.1/generated/torch.nn.RNN.html\">Documenation</a>\n",
        "<br />\n",
        "Example: <a href=\"https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\">Example</a>."
      ],
      "metadata": {
        "id": "xZtyQZH20EIp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
        "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        combined = torch.cat((input, hidden), 1)\n",
        "        hidden = self.i2h(combined)\n",
        "        output = self.i2o(combined)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)\n",
        "\n",
        "n_hidden = 128\n",
        "n_letters = 100\n",
        "n_categories = 10\n",
        "\n",
        "rnn = RNN(n_letters, n_hidden, n_categories)"
      ],
      "metadata": {
        "id": "NabFjMFizRMc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id=\"2\"><h2>LSTM model implementation</h2></div>\n",
        "Pytorch documentation: <a href=\"https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\">Documenation</a>\n"
      ],
      "metadata": {
        "id": "Yb9Uu3qG1TK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM1(nn.Module):\n",
        "    def __init__(self, num_classes, input_size, hidden_size, num_layers, seq_length):\n",
        "        super(LSTM1, self).__init__()\n",
        "        self.num_classes = num_classes #number of classes\n",
        "        self.num_layers = num_layers #number of layers\n",
        "        self.input_size = input_size #input size\n",
        "        self.hidden_size = hidden_size #hidden state\n",
        "        self.seq_length = seq_length #sequence length\n",
        "\n",
        "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\n",
        "                          num_layers=num_layers, batch_first=True) #lstm\n",
        "        self.fc_1 =  nn.Linear(hidden_size, 128) #fully connected 1\n",
        "        self.fc = nn.Linear(128, num_classes) #fully connected last layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self,x):\n",
        "        h_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #hidden state\n",
        "        c_0 = Variable(torch.zeros(self.num_layers, x.size(0), self.hidden_size)) #internal state\n",
        "        # Propagate input through LSTM\n",
        "        output, (hn, cn) = self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
        "        hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
        "        out = self.relu(hn)\n",
        "        out = self.fc_1(out) #first Dense\n",
        "        out = self.relu(out) #relu\n",
        "        out = self.fc(out) #Final Output\n",
        "        return out\n",
        " "
      ],
      "metadata": {
        "id": "ldkITTk5zyxG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id=\"3\"><h2>GRU model implementation</h2></div>\n",
        "\n",
        "For more information please read: \n",
        "https://pytorch.org/docs/1.9.1/generated/torch.nn.GRU.html"
      ],
      "metadata": {
        "id": "rs6X-I-91hey"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUNet(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, n_layers, drop_prob=0.2):\n",
        "        super(GRUNet, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, n_layers, batch_first=True, dropout=drop_prob)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x, h):\n",
        "        out, h = self.gru(x, h)\n",
        "        out = self.fc(self.relu(out[:,-1]))\n",
        "        return out, h\n",
        "    \n",
        "    def init_hidden(self, batch_size):\n",
        "        weight = next(self.parameters()).data\n",
        "        hidden = weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device)\n",
        "        return hidden"
      ],
      "metadata": {
        "id": "3Pg8J-D61LWN"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div id=\"4\"><h2>AutoEncoder model implementation</h2></div>\n",
        "\n",
        "For more information please read: \n",
        "https://www.geeksforgeeks.org/implementing-an-autoencoder-in-pytorch/\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "EEh7kwlc2MIn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AE(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "          \n",
        "        # Building an linear encoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # 784 ==> 9\n",
        "        self.encoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(28 * 28, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 36),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(36, 18),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(18, 9)\n",
        "        )\n",
        "          \n",
        "        # Building an linear decoder with Linear\n",
        "        # layer followed by Relu activation function\n",
        "        # The Sigmoid activation function\n",
        "        # outputs the value between 0 and 1\n",
        "        # 9 ==> 784\n",
        "        self.decoder = torch.nn.Sequential(\n",
        "            torch.nn.Linear(9, 18),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(18, 36),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(36, 64),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(64, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 28 * 28),\n",
        "            torch.nn.Sigmoid()\n",
        "        )\n",
        "  \n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "Ifx-dXf62MRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<hr />\n",
        "\n",
        "### About the author\n",
        "\n",
        "`Name`: Erfan Asadi\n",
        "<br />\n",
        "`Email`: erfanasadi.ce@gmail.com\n",
        "<br />\n",
        "`Linkedin`: https://www.linkedin.com/in/erfan-asadi-9b64b9163/\n",
        "<br />\n",
        "`GitHub`: https://github.com/ErfanAsadi\n",
        "<br />\n",
        "<hr />"
      ],
      "metadata": {
        "id": "QCbMymLQ2lPI"
      }
    }
  ]
}